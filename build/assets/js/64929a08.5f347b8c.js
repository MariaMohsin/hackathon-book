"use strict";(self.webpackChunkai_native_book=self.webpackChunkai_native_book||[]).push([[42],{412:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"vision-language-action/training-vla-models","title":"Training VLA Models","description":"Data Collection","source":"@site/docs/vision-language-action/training-vla-models.md","sourceDirName":"vision-language-action","slug":"/vision-language-action/training-vla-models","permalink":"/docs/vision-language-action/training-vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/docs/docs/vision-language-action/training-vla-models.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Training VLA Models","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"VLA Fundamentals","permalink":"/docs/vision-language-action/vla-fundamentals"},"next":{"title":"Deploying VLA on Robots","permalink":"/docs/vision-language-action/deploying-vla"}}');var s=r(4848),i=r(8453);const a={title:"Training VLA Models",sidebar_position:2},o="Training Vision-Language-Action Models",l={},d=[{value:"Data Collection",id:"data-collection",level:2},{value:"Teleoperation",id:"teleoperation",level:3},{value:"Synthetic Data",id:"synthetic-data",level:3},{value:"Dataset Structure",id:"dataset-structure",level:2},{value:"Model Architecture",id:"model-architecture",level:2},{value:"Vision Encoder",id:"vision-encoder",level:3},{value:"Language Encoder",id:"language-encoder",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:3},{value:"Action Decoder",id:"action-decoder",level:3},{value:"Training",id:"training",level:2},{value:"Full VLA Model",id:"full-vla-model",level:3},{value:"Evaluating VLA Models",id:"evaluating-vla-models",level:2},{value:"Success Metrics",id:"success-metrics",level:3},{value:"Optimization and Deployment",id:"optimization-and-deployment",level:2},{value:"Model Quantization",id:"model-quantization",level:3},{value:"Knowledge Distillation",id:"knowledge-distillation",level:3},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"training-vision-language-action-models",children:"Training Vision-Language-Action Models"})}),"\n",(0,s.jsx)(n.h2,{id:"data-collection",children:"Data Collection"}),"\n",(0,s.jsx)(n.h3,{id:"teleoperation",children:"Teleoperation"}),"\n",(0,s.jsx)(n.p,{children:"Collect robot demonstrations through human control:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class TeleoperationDataCollector:\r\n    def __init__(self, robot):\r\n        self.robot = robot\r\n        self.data = []\r\n    \r\n    def collect_episode(self, instruction):\r\n        \"\"\"Collect one demonstration episode\"\"\"\r\n        \r\n        episode = {\r\n            'instruction': instruction,\r\n            'frames': [],\r\n            'actions': [],\r\n            'rewards': []\r\n        }\r\n        \r\n        print(f\"Collecting data for: {instruction}\")\r\n        print(\"Use gamepad to teleoperate robot. Press START when done.\")\r\n        \r\n        while not self.is_episode_done():\r\n            # Capture frame\r\n            frame = self.robot.camera.get_rgb()\r\n            episode['frames'].append(frame)\r\n            \r\n            # Get human action\r\n            action = self.gamepad_controller.get_action()\r\n            episode['actions'].append(action)\r\n            \r\n            # Execute action on robot\r\n            self.robot.execute_action(action)\r\n            \r\n            # Get reward (success signal)\r\n            reward = self.evaluate_success(instruction)\r\n            episode['rewards'].append(reward)\r\n        \r\n        self.data.append(episode)\r\n        return episode\r\n    \r\n    def save_dataset(self, path):\r\n        \"\"\"Save collected data to disk\"\"\"\r\n        import json\r\n        with open(path, 'w') as f:\r\n            json.dump(self.data, f)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"synthetic-data",children:"Synthetic Data"}),"\n",(0,s.jsx)(n.p,{children:"Generate data from simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SyntheticDataGenerator:\r\n    def __init__(self, sim_world):\r\n        self.sim_world = sim_world\r\n        self.data = []\r\n    \r\n    def generate_trajectories(self, num_episodes=1000):\r\n        """Generate random trajectories in simulation"""\r\n        \r\n        for episode_idx in range(num_episodes):\r\n            # Randomize scene\r\n            self.sim_world.randomize_objects()\r\n            instruction = self._generate_instruction()\r\n            \r\n            # Generate trajectory\r\n            trajectory = self._generate_trajectory_for_instruction(instruction)\r\n            \r\n            self.data.append({\r\n                \'instruction\': instruction,\r\n                \'trajectory\': trajectory,\r\n                \'sim_data\': True\r\n            })\r\n        \r\n        return self.data\r\n    \r\n    def _generate_trajectory_for_instruction(self, instruction):\r\n        """Use rule-based policy to generate trajectory"""\r\n        if "pick up" in instruction.lower():\r\n            return self._pick_and_place_trajectory()\r\n        elif "place" in instruction.lower():\r\n            return self._placement_trajectory()\r\n        # ... more rules\n'})}),"\n",(0,s.jsx)(n.h2,{id:"dataset-structure",children:"Dataset Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"dataset/\r\n\u251c\u2500\u2500 instructions/\r\n\u2502   \u251c\u2500\u2500 pick_cup.txt\r\n\u2502   \u251c\u2500\u2500 open_drawer.txt\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 episodes/\r\n\u2502   \u251c\u2500\u2500 episode_0001/\r\n\u2502   \u2502   \u251c\u2500\u2500 frames/\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 frame_0000.jpg\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 frame_0001.jpg\r\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\r\n\u2502   \u2502   \u251c\u2500\u2500 actions.json      # Joint angles, gripper state\r\n\u2502   \u2502   \u251c\u2500\u2500 metadata.json      # Task info, success signal\r\n\u2502   \u2502   \u2514\u2500\u2500 instruction.txt    # Natural language instruction\r\n\u2502   \u251c\u2500\u2500 episode_0002/\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u2514\u2500\u2500 metadata.json              # Dataset statistics\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"vision-encoder",children:"Vision Encoder"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nfrom torchvision import models\r\n\r\nclass VisionEncoder(nn.Module):\r\n    def __init__(self, output_dim=256):\r\n        super().__init__()\r\n        \r\n        # Use pre-trained ResNet-50\r\n        self.backbone = models.resnet50(pretrained=True)\r\n        \r\n        # Remove classification head\r\n        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\r\n        \r\n        # Add projection head\r\n        self.projection = nn.Linear(2048, output_dim)\r\n    \r\n    def forward(self, x):\r\n        features = self.backbone(x)\r\n        features = features.squeeze(-1).squeeze(-1)\r\n        embeddings = self.projection(features)\r\n        return embeddings\n"})}),"\n",(0,s.jsx)(n.h3,{id:"language-encoder",children:"Language Encoder"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from transformers import AutoTokenizer, AutoModel\r\n\r\nclass LanguageEncoder(nn.Module):\r\n    def __init__(self, model_name=\"bert-base-uncased\", output_dim=256):\r\n        super().__init__()\r\n        \r\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        self.model = AutoModel.from_pretrained(model_name)\r\n        \r\n        # Projection to match vision embedding dimension\r\n        self.projection = nn.Linear(768, output_dim)\r\n    \r\n    def forward(self, text):\r\n        tokens = self.tokenizer(text, return_tensors='pt', padding=True)\r\n        outputs = self.model(**tokens)\r\n        \r\n        # Use [CLS] token representation\r\n        cls_embedding = outputs.last_hidden_state[:, 0, :]\r\n        embeddings = self.projection(cls_embedding)\r\n        return embeddings\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultiModalFusion(nn.Module):\r\n    def __init__(self, embedding_dim=256, num_heads=4):\r\n        super().__init__()\r\n        \r\n        self.cross_attention = nn.MultiheadAttention(\r\n            embedding_dim, num_heads, batch_first=True\r\n        )\r\n    \r\n    def forward(self, visual_embeds, language_embeds):\r\n        # Cross-attention between vision and language\r\n        fused, _ = self.cross_attention(\r\n            visual_embeds,\r\n            language_embeds,\r\n            language_embeds\r\n        )\r\n        return fused\n"})}),"\n",(0,s.jsx)(n.h3,{id:"action-decoder",children:"Action Decoder"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ActionDecoder(nn.Module):\r\n    def __init__(self, input_dim=256, action_dim=7, num_waypoints=5):\r\n        super().__init__()\r\n        \r\n        self.action_dim = action_dim  # 6 joints + 1 gripper\r\n        self.num_waypoints = num_waypoints\r\n        \r\n        self.layers = nn.Sequential(\r\n            nn.Linear(input_dim, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, num_waypoints * action_dim)\r\n        )\r\n    \r\n    def forward(self, fused_embedding):\r\n        action_sequence = self.layers(fused_embedding)\r\n        \r\n        # Reshape to [num_waypoints, action_dim]\r\n        actions = action_sequence.reshape(-1, self.num_waypoints, self.action_dim)\r\n        \r\n        return actions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,s.jsx)(n.h3,{id:"full-vla-model",children:"Full VLA Model"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch.optim as optim\r\n\r\nclass VLATrainer:\r\n    def __init__(self, model, learning_rate=1e-4):\r\n        self.model = model\r\n        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n        self.criterion = nn.MSELoss()\r\n    \r\n    def train_epoch(self, train_loader):\r\n        self.model.train()\r\n        total_loss = 0\r\n        \r\n        for batch in train_loader:\r\n            images, instructions, actions = batch\r\n            \r\n            # Forward pass\r\n            predicted_actions = self.model(images, instructions)\r\n            \r\n            # Compute loss\r\n            loss = self.criterion(predicted_actions, actions)\r\n            \r\n            # Backward pass\r\n            self.optimizer.zero_grad()\r\n            loss.backward()\r\n            self.optimizer.step()\r\n            \r\n            total_loss += loss.item()\r\n        \r\n        return total_loss / len(train_loader)\r\n    \r\n    def train(self, train_loader, val_loader, epochs=100):\r\n        best_val_loss = float(\'inf\')\r\n        \r\n        for epoch in range(epochs):\r\n            train_loss = self.train_epoch(train_loader)\r\n            val_loss = self.validate(val_loader)\r\n            \r\n            print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")\r\n            \r\n            if val_loss < best_val_loss:\r\n                best_val_loss = val_loss\r\n                self.save_checkpoint(f"best_model.pth")\r\n    \r\n    def validate(self, val_loader):\r\n        self.model.eval()\r\n        total_loss = 0\r\n        \r\n        with torch.no_grad():\r\n            for batch in val_loader:\r\n                images, instructions, actions = batch\r\n                predicted_actions = self.model(images, instructions)\r\n                loss = self.criterion(predicted_actions, actions)\r\n                total_loss += loss.item()\r\n        \r\n        return total_loss / len(val_loader)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"evaluating-vla-models",children:"Evaluating VLA Models"}),"\n",(0,s.jsx)(n.h3,{id:"success-metrics",children:"Success Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class VLAEvaluator:\r\n    def __init__(self, robot):\r\n        self.robot = robot\r\n    \r\n    def evaluate_success(self, instruction, predicted_actions, max_steps=100):\r\n        \"\"\"Execute predicted actions and check if task succeeded\"\"\"\r\n        \r\n        success = False\r\n        steps = 0\r\n        \r\n        for action in predicted_actions:\r\n            self.robot.execute_action(action)\r\n            steps += 1\r\n            \r\n            # Check success condition\r\n            success = self._check_success_condition(instruction)\r\n            if success:\r\n                break\r\n            \r\n            if steps > max_steps:\r\n                break\r\n        \r\n        return {\r\n            'success': success,\r\n            'steps': steps,\r\n            'efficiency': steps / max_steps\r\n        }\r\n    \r\n    def evaluate_dataset(self, test_episodes):\r\n        \"\"\"Evaluate on entire test set\"\"\"\r\n        \r\n        results = []\r\n        success_rate = 0\r\n        \r\n        for instruction, actions in test_episodes:\r\n            result = self.evaluate_success(instruction, actions)\r\n            results.append(result)\r\n            success_rate += result['success']\r\n        \r\n        success_rate /= len(test_episodes)\r\n        \r\n        return {\r\n            'success_rate': success_rate,\r\n            'results': results\r\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"optimization-and-deployment",children:"Optimization and Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch.quantization\r\n\r\ndef quantize_model(model):\r\n    """Convert to lower precision for faster inference"""\r\n    \r\n    # Prepare model\r\n    model.qconfig = torch.quantization.get_default_qat_qconfig(\'fbgemm\')\r\n    torch.quantization.prepare_qat(model, inplace=True)\r\n    \r\n    # Convert to quantized model\r\n    torch.quantization.convert(model, inplace=True)\r\n    \r\n    return model\n'})}),"\n",(0,s.jsx)(n.h3,{id:"knowledge-distillation",children:"Knowledge Distillation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def distill_to_smaller_model(teacher_model, teacher_output):\r\n    """Train smaller student model to mimic teacher"""\r\n    \r\n    student_model = VLAModel(smaller=True)\r\n    teacher_model.eval()\r\n    \r\n    for images, instructions in train_loader:\r\n        # Get teacher predictions\r\n        with torch.no_grad():\r\n            teacher_actions = teacher_model(images, instructions)\r\n        \r\n        # Train student to match teacher\r\n        student_actions = student_model(images, instructions)\r\n        loss = nn.KLDivLoss()(student_actions, teacher_actions)\r\n        loss.backward()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["Learn about ",(0,s.jsx)(n.a,{href:"deploying-vla",children:"Deploying VLA on Robots"})," to run your trained models on real hardware."]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);