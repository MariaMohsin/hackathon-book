"use strict";(self.webpackChunkai_native_book=self.webpackChunkai_native_book||[]).push([[704],{6786:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"vision-language-action/deploying-vla","title":"Deploying VLA on Robots","description":"Deployment Pipeline","source":"@site/docs/vision-language-action/deploying-vla.md","sourceDirName":"vision-language-action","slug":"/vision-language-action/deploying-vla","permalink":"/docs/vision-language-action/deploying-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/docs/docs/vision-language-action/deploying-vla.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Deploying VLA on Robots","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Training VLA Models","permalink":"/docs/vision-language-action/training-vla-models"},"next":{"title":"Welcome to the Physical AI & Humanoid Robotics Book","permalink":"/docs/intro"}}');var i=r(4848),o=r(8453);const s={title:"Deploying VLA on Robots",sidebar_position:3},a="Deploying Vision-Language-Action Models on Robots",l={},c=[{value:"Deployment Pipeline",id:"deployment-pipeline",level:2},{value:"Model Optimization",id:"model-optimization",level:2},{value:"1. Convert to ONNX",id:"1-convert-to-onnx",level:3},{value:"2. Quantization",id:"2-quantization",level:3},{value:"3. Model Size Comparison",id:"3-model-size-comparison",level:3},{value:"Edge Device Setup",id:"edge-device-setup",level:2},{value:"NVIDIA Jetson Setup",id:"nvidia-jetson-setup",level:3},{value:"Running Inference on Jetson",id:"running-inference-on-jetson",level:3},{value:"Real-Time Inference",id:"real-time-inference",level:2},{value:"ROS 2 Node for VLA Inference",id:"ros-2-node-for-vla-inference",level:3},{value:"Action Execution",id:"action-execution",level:2},{value:"Converting Model Output to Robot Commands",id:"converting-model-output-to-robot-commands",level:3},{value:"Safety and Error Handling",id:"safety-and-error-handling",level:2},{value:"Safety Constraints",id:"safety-constraints",level:3},{value:"Monitoring and Logging",id:"monitoring-and-logging",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"deploying-vision-language-action-models-on-robots",children:"Deploying Vision-Language-Action Models on Robots"})}),"\n",(0,i.jsx)(e.h2,{id:"deployment-pipeline",children:"Deployment Pipeline"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Trained Model \u2192 Optimization \u2192 Edge Device \u2192 Real Robot\r\n     \u2193              \u2193               \u2193           \u2193\r\n  PyTorch    Quantization    Jetson/CPU   Execute Actions\n"})}),"\n",(0,i.jsx)(e.h2,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"1-convert-to-onnx",children:"1. Convert to ONNX"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.onnx\r\n\r\ndef export_to_onnx(model, example_image, example_text):\r\n    \"\"\"Export PyTorch model to ONNX format\"\"\"\r\n    \r\n    torch.onnx.export(\r\n        model,\r\n        (example_image, example_text),\r\n        \"vla_model.onnx\",\r\n        opset_version=11,\r\n        do_constant_folding=True,\r\n        input_names=['image', 'text'],\r\n        output_names=['actions'],\r\n        dynamic_axes={\r\n            'image': {0: 'batch_size'},\r\n            'text': {0: 'batch_size'},\r\n            'actions': {0: 'batch_size'}\r\n        }\r\n    )\r\n    \r\n    return \"vla_model.onnx\"\n"})}),"\n",(0,i.jsx)(e.h3,{id:"2-quantization",children:"2. Quantization"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import onnx\r\nimport onnxruntime as rt\r\nfrom onnxruntime.quantization import quantize_dynamic\r\n\r\ndef quantize_onnx_model(onnx_path):\r\n    """Quantize ONNX model for faster inference"""\r\n    \r\n    quantized_path = "vla_model_quantized.onnx"\r\n    \r\n    quantize_dynamic(\r\n        onnx_path,\r\n        quantized_path,\r\n        weight_type=QuantType.QUInt8\r\n    )\r\n    \r\n    # Verify quantized model\r\n    model = onnx.load(quantized_path)\r\n    onnx.checker.check_model(model)\r\n    \r\n    return quantized_path\n'})}),"\n",(0,i.jsx)(e.h3,{id:"3-model-size-comparison",children:"3. Model Size Comparison"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import os\r\n\r\ndef compare_model_sizes(fp32_path, quantized_path):\r\n    """Compare model sizes"""\r\n    \r\n    fp32_size = os.path.getsize(fp32_path) / (1024**2)  # MB\r\n    quantized_size = os.path.getsize(quantized_path) / (1024**2)\r\n    \r\n    print(f"FP32 Model: {fp32_size:.1f} MB")\r\n    print(f"Quantized Model: {quantized_size:.1f} MB")\r\n    print(f"Reduction: {(1 - quantized_size/fp32_size)*100:.1f}%")\r\n    \r\n    return {\r\n        \'original\': fp32_size,\r\n        \'quantized\': quantized_size,\r\n        \'reduction_percent\': (1 - quantized_size/fp32_size)*100\r\n    }\n'})}),"\n",(0,i.jsx)(e.h2,{id:"edge-device-setup",children:"Edge Device Setup"}),"\n",(0,i.jsx)(e.h3,{id:"nvidia-jetson-setup",children:"NVIDIA Jetson Setup"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Install JetPack\r\nsudo apt-get update\r\nsudo apt-get install nvidia-jetpack\r\n\r\n# Verify GPU\r\nnvidia-smi\r\n\r\n# Install TensorRT for optimized inference\r\nsudo apt-get install python3-tensorrt\r\n\r\n# Install ROS 2\r\nsudo apt-get install ros-humble-ros-core\n"})}),"\n",(0,i.jsx)(e.h3,{id:"running-inference-on-jetson",children:"Running Inference on Jetson"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import tensorrt as trt\r\nimport numpy as np\r\n\r\nclass JetsonVLAInference:\r\n    def __init__(self, engine_path):\r\n        self.logger = trt.Logger(trt.Logger.INFO)\r\n        \r\n        with open(engine_path, \'rb\') as f:\r\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\r\n        \r\n        self.context = self.engine.create_execution_context()\r\n    \r\n    def infer(self, image, instruction):\r\n        """Run inference on Jetson"""\r\n        \r\n        # Prepare input\r\n        image_tensor = np.ascontiguousarray(image)\r\n        instruction_tensor = np.ascontiguousarray(instruction)\r\n        \r\n        # Allocate output\r\n        output = np.empty(self.get_output_shape(), dtype=np.float32)\r\n        \r\n        # Execute\r\n        self.context.execute_v2([\r\n            image_tensor.data_ptr(),\r\n            instruction_tensor.data_ptr(),\r\n            output.data_ptr()\r\n        ])\r\n        \r\n        return output\r\n    \r\n    def get_output_shape(self):\r\n        return (1, 5, 7)  # [batch, num_waypoints, action_dim]\n'})}),"\n",(0,i.jsx)(e.h2,{id:"real-time-inference",children:"Real-Time Inference"}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-node-for-vla-inference",children:"ROS 2 Node for VLA Inference"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String, Float32MultiArray\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\n\r\nclass VLAInferenceNode(Node):\r\n    def __init__(self, model_path):\r\n        super().__init__(\'vla_inference\')\r\n        \r\n        # Load model\r\n        self.model = self.load_model(model_path)\r\n        self.bridge = CvBridge()\r\n        \r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'camera/image_raw\', self.image_callback, 10\r\n        )\r\n        self.instruction_sub = self.create_subscription(\r\n            String, \'vla/instruction\', self.instruction_callback, 10\r\n        )\r\n        self.action_pub = self.create_publisher(\r\n            Float32MultiArray, \'vla/actions\', 10\r\n        )\r\n        \r\n        self.current_image = None\r\n        self.current_instruction = None\r\n        \r\n        # Timer for inference\r\n        self.timer = self.create_timer(0.1, self.run_inference)\r\n    \r\n    def load_model(self, model_path):\r\n        """Load VLA model with your framework"""\r\n        import onnxruntime as rt\r\n        session = rt.InferenceSession(model_path)\r\n        return session\r\n    \r\n    def image_callback(self, msg):\r\n        """Receive camera image"""\r\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n    \r\n    def instruction_callback(self, msg):\r\n        """Receive natural language instruction"""\r\n        self.current_instruction = msg.data\r\n        self.get_logger().info(f"Instruction: {self.current_instruction}")\r\n    \r\n    def run_inference(self):\r\n        """Run VLA inference when both image and instruction available"""\r\n        \r\n        if self.current_image is None or self.current_instruction is None:\r\n            return\r\n        \r\n        try:\r\n            # Preprocess image\r\n            input_image = self.preprocess_image(self.current_image)\r\n            \r\n            # Run inference\r\n            start_time = self.get_clock().now()\r\n            \r\n            output = self.model.run(\r\n                None,\r\n                {\r\n                    \'image\': input_image,\r\n                    \'text\': np.array([self.current_instruction])\r\n                }\r\n            )\r\n            \r\n            inference_time = (self.get_clock().now() - start_time).nanoseconds / 1e9\r\n            \r\n            # Extract action sequence\r\n            actions = output[0]\r\n            \r\n            # Publish actions\r\n            action_msg = Float32MultiArray()\r\n            action_msg.data = actions.flatten().tolist()\r\n            self.action_pub.publish(action_msg)\r\n            \r\n            self.get_logger().info(\r\n                f"Inference time: {inference_time*1000:.1f}ms, "\r\n                f"Actions: {actions.shape}"\r\n            )\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f"Inference error: {e}")\r\n    \r\n    def preprocess_image(self, image):\r\n        """Prepare image for model input"""\r\n        \r\n        # Resize to model input size\r\n        resized = cv2.resize(image, (224, 224))\r\n        \r\n        # Normalize\r\n        normalized = resized.astype(np.float32) / 255.0\r\n        \r\n        # Add batch dimension\r\n        batched = np.expand_dims(normalized, axis=0)\r\n        \r\n        return batched\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VLAInferenceNode("vla_model_optimized.onnx")\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"action-execution",children:"Action Execution"}),"\n",(0,i.jsx)(e.h3,{id:"converting-model-output-to-robot-commands",children:"Converting Model Output to Robot Commands"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ActionExecutor:\r\n    def __init__(self, robot):\r\n        self.robot = robot\r\n    \r\n    def execute_vla_actions(self, action_sequence, instruction):\r\n        """Execute sequence of actions from VLA model"""\r\n        \r\n        self.get_logger().info(f"Executing: {instruction}")\r\n        \r\n        for idx, waypoint_action in enumerate(action_sequence):\r\n            # Waypoint is [joint1, joint2, ..., joint6, gripper]\r\n            joint_angles = waypoint_action[:6]\r\n            gripper_state = waypoint_action[6]\r\n            \r\n            self.execute_waypoint(joint_angles, gripper_state)\r\n            \r\n            # Check for early termination\r\n            if self.check_success_condition(instruction):\r\n                self.get_logger().info("Task completed successfully!")\r\n                return True\r\n            \r\n            # Wait between waypoints\r\n            time.sleep(0.5)\r\n        \r\n        return False\r\n    \r\n    def execute_waypoint(self, joint_angles, gripper_state):\r\n        """Move to single waypoint"""\r\n        \r\n        # Send to ROS 2 controller\r\n        trajectory_msg = FollowJointTrajectoryGoal()\r\n        trajectory_msg.trajectory.header.stamp = self.get_clock().now().to_msg()\r\n        \r\n        # Create trajectory point\r\n        point = JointTrajectoryPoint()\r\n        point.positions = joint_angles\r\n        point.velocities = [0.0] * len(joint_angles)\r\n        point.time_from_start = Duration(sec=1, nanosec=0).to_msg()\r\n        \r\n        trajectory_msg.trajectory.points.append(point)\r\n        \r\n        # Send trajectory\r\n        self.action_client.send_goal(trajectory_msg)\r\n        \r\n        # Control gripper\r\n        if gripper_state > 0.5:\r\n            self.robot.gripper.close()\r\n        else:\r\n            self.robot.gripper.open()\r\n    \r\n    def check_success_condition(self, instruction):\r\n        """Check if task completed successfully"""\r\n        \r\n        # Example: for "pick up" tasks, check if object is griped\r\n        if "pick" in instruction.lower():\r\n            return self.robot.gripper.is_gripping()\r\n        \r\n        # Add more task-specific checks\r\n        return False\n'})}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-error-handling",children:"Safety and Error Handling"}),"\n",(0,i.jsx)(e.h3,{id:"safety-constraints",children:"Safety Constraints"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class SafeVLAExecutor:\r\n    def __init__(self, robot, joint_limits, velocity_limits):\r\n        self.robot = robot\r\n        self.joint_limits = joint_limits\r\n        self.velocity_limits = velocity_limits\r\n    \r\n    def validate_action(self, action):\r\n        """Check action safety before execution"""\r\n        \r\n        joint_angles = action[:6]\r\n        \r\n        # Check joint limits\r\n        for i, angle in enumerate(joint_angles):\r\n            if angle < self.joint_limits[i][\'min\'] or angle > self.joint_limits[i][\'max\']:\r\n                self.get_logger().warn(\r\n                    f"Joint {i} angle {angle} exceeds limits. Clamping."\r\n                )\r\n                joint_angles[i] = np.clip(\r\n                    angle,\r\n                    self.joint_limits[i][\'min\'],\r\n                    self.joint_limits[i][\'max\']\r\n                )\r\n        \r\n        return joint_angles\r\n    \r\n    def execute_with_safety(self, action_sequence):\r\n        """Execute with safety checks"""\r\n        \r\n        for action in action_sequence:\r\n            # Validate action\r\n            safe_action = self.validate_action(action)\r\n            \r\n            # Check for obstacles\r\n            if self.check_collision_risk(safe_action):\r\n                self.get_logger().error("Collision detected! Stopping.")\r\n                self.robot.stop()\r\n                return False\r\n            \r\n            # Execute\r\n            self.robot.execute_action(safe_action)\r\n    \r\n    def check_collision_risk(self, action):\r\n        """Use collision detection to verify safety"""\r\n        \r\n        # Move to action in simulation\r\n        self.collision_simulator.predict_state(action)\r\n        \r\n        # Check for collisions\r\n        collisions = self.collision_detector.get_collisions()\r\n        \r\n        return len(collisions) > 0\n'})}),"\n",(0,i.jsx)(e.h2,{id:"monitoring-and-logging",children:"Monitoring and Logging"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class VLAMonitor:\r\n    def __init__(self):\r\n        self.metrics = {\r\n            'inference_time': [],\r\n            'success_count': 0,\r\n            'failure_count': 0,\r\n            'instructions_executed': []\r\n        }\r\n    \r\n    def log_execution(self, instruction, success, inference_time):\r\n        \"\"\"Log VLA execution\"\"\"\r\n        \r\n        self.metrics['inference_time'].append(inference_time)\r\n        self.metrics['instructions_executed'].append(instruction)\r\n        \r\n        if success:\r\n            self.metrics['success_count'] += 1\r\n        else:\r\n            self.metrics['failure_count'] += 1\r\n        \r\n        # Calculate statistics\r\n        avg_inference = np.mean(self.metrics['inference_time'])\r\n        success_rate = (self.metrics['success_count'] / \r\n                       (self.metrics['success_count'] + self.metrics['failure_count']))\r\n        \r\n        print(f\"Success Rate: {success_rate*100:.1f}%\")\r\n        print(f\"Avg Inference: {avg_inference*1000:.1f}ms\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(e.p,{children:"Explore real-world applications and advanced deployment scenarios in your specific robotics domain."})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);