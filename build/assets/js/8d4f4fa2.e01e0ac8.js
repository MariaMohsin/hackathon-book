"use strict";(self.webpackChunkai_native_book=self.webpackChunkai_native_book||[]).push([[458],{7571:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"vision-language-action/vla-fundamentals","title":"VLA Fundamentals","description":"What is VLA?","source":"@site/docs/vision-language-action/vla-fundamentals.md","sourceDirName":"vision-language-action","slug":"/vision-language-action/vla-fundamentals","permalink":"/docs/vision-language-action/vla-fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/docs/docs/vision-language-action/vla-fundamentals.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"VLA Fundamentals","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action (VLA)","permalink":"/docs/category/vision-language-action-vla"},"next":{"title":"Training VLA Models","permalink":"/docs/vision-language-action/training-vla-models"}}');var r=i(4848),a=i(8453);const t={title:"VLA Fundamentals",sidebar_position:1},l="Vision-Language-Action (VLA) Fundamentals",o={},d=[{value:"What is VLA?",id:"what-is-vla",level:2},{value:"The VLA Pipeline",id:"the-vla-pipeline",level:2},{value:"VLA Advantages",id:"vla-advantages",level:2},{value:"1. Intuitive Instruction Format",id:"1-intuitive-instruction-format",level:3},{value:"2. Few-Shot Learning",id:"2-few-shot-learning",level:3},{value:"3. Generalization",id:"3-generalization",level:3},{value:"VLA Model Architectures",id:"vla-model-architectures",level:2},{value:"Vision Transformer (ViT)",id:"vision-transformer-vit",level:3},{value:"Language Model (LLM)",id:"language-model-llm",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:3},{value:"VLA Action Output",id:"vla-action-output",level:2},{value:"Joint-Space Actions",id:"joint-space-actions",level:3},{value:"Task-Space Actions",id:"task-space-actions",level:3},{value:"Waypoint Sequences",id:"waypoint-sequences",level:3},{value:"Popular VLA Models",id:"popular-vla-models",level:2},{value:"RT-1 (Robotics Transformer-1)",id:"rt-1-robotics-transformer-1",level:3},{value:"RT-2 (Robotics Transformer-2)",id:"rt-2-robotics-transformer-2",level:3},{value:"Open-Source Alternatives",id:"open-source-alternatives",level:3},{value:"VLA vs Traditional Robotics",id:"vla-vs-traditional-robotics",level:2},{value:"Current Limitations",id:"current-limitations",level:2},{value:"1. Computational Requirements",id:"1-computational-requirements",level:3},{value:"2. Real-Time Performance",id:"2-real-time-performance",level:3},{value:"3. Sim-to-Real Gap",id:"3-sim-to-real-gap",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"1. Pick and Place",id:"1-pick-and-place",level:3},{value:"2. Manipulation Tasks",id:"2-manipulation-tasks",level:3},{value:"3. Assembly Tasks",id:"3-assembly-tasks",level:3},{value:"4. Mobile Manipulation",id:"4-mobile-manipulation",level:3},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vision-language-action-vla-fundamentals",children:"Vision-Language-Action (VLA) Fundamentals"})}),"\n",(0,r.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,r.jsx)(n.p,{children:"Vision-Language-Action (VLA) is an AI paradigm that combines:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Computer vision and image understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language"}),": Natural language processing and LLMs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": Robot control and manipulation"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"VLA systems can understand visual scenes, interpret natural language instructions, and execute corresponding robot actions."}),"\n",(0,r.jsx)(n.h2,{id:"the-vla-pipeline",children:"The VLA Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Visual Input (Camera) \u2500\u2500\u2192 Vision Encoder \u2500\u2500\u2192 Multi-Modal Fusion \u2500\u2500\u2192 Action Decoder \u2500\u2500\u2192 Robot Command\r\n    \u2193                          \u2193                     \u2193                    \u2193\r\nText Input (Instruction) \u2500\u2500\u2192 Language Encoder \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vla-advantages",children:"VLA Advantages"}),"\n",(0,r.jsx)(n.h3,{id:"1-intuitive-instruction-format",children:"1. Intuitive Instruction Format"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User: "Pick up the red cube and place it on the table"\r\n\r\nVLA System understands:\r\n- Visual: Where is the red cube?\r\n- Language: What is the task?\r\n- Action: What arm/gripper commands?\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-few-shot-learning",children:"2. Few-Shot Learning"}),"\n",(0,r.jsx)(n.p,{children:"Train with few examples:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# With traditional robotics: requires 1000s of examples\r\n# With VLA: can learn from 10-50 demonstrations\r\ndemonstrations = [\r\n    {\r\n        'image': camera_frame,\r\n        'instruction': \"Pick up the block\",\r\n        'actions': [arm_position, gripper_command]\r\n    },\r\n    # ... more examples\r\n]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-generalization",children:"3. Generalization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Trained on:\r\n# - Red cube, wooden table\r\n# - Blue sphere, metal table\r\n# - Green block, plastic table\r\n\r\n# Can generalize to:\r\n# - Yellow cylinder, marble table (never seen before)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vla-model-architectures",children:"VLA Model Architectures"}),"\n",(0,r.jsx)(n.h3,{id:"vision-transformer-vit",children:"Vision Transformer (ViT)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Divides image into patches"}),"\n",(0,r.jsx)(n.li,{children:"Applies transformer attention"}),"\n",(0,r.jsx)(n.li,{children:"Excellent for image understanding"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"language-model-llm",children:"Language Model (LLM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"GPT-style autoregressive models"}),"\n",(0,r.jsx)(n.li,{children:"Claude, GPT-4, Llama"}),"\n",(0,r.jsx)(n.li,{children:"Generates action sequences"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class VLAModel:\r\n    def __init__(self):\r\n        self.vision_encoder = VisionTransformer()  # Image \u2192 embeddings\r\n        self.language_encoder = LanguageModel()    # Text \u2192 embeddings\r\n        self.fusion_layer = CrossAttention()       # Combine modalities\r\n        self.action_decoder = ActionDecoder()      # Embeddings \u2192 robot actions\r\n    \r\n    def forward(self, image, instruction):\r\n        # Encode visual and language inputs\r\n        visual_features = self.vision_encoder(image)\r\n        language_features = self.language_encoder(instruction)\r\n        \r\n        # Fuse multi-modal information\r\n        fused = self.fusion_layer(visual_features, language_features)\r\n        \r\n        # Generate robot actions\r\n        actions = self.action_decoder(fused)\r\n        \r\n        return actions\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vla-action-output",children:"VLA Action Output"}),"\n",(0,r.jsx)(n.h3,{id:"joint-space-actions",children:"Joint-Space Actions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Direct joint commands\r\nactions = {\r\n    'shoulder_pan': 0.5,      # radians\r\n    'shoulder_lift': 1.2,\r\n    'elbow': -0.3,\r\n    'wrist_1': 0.0,\r\n    'wrist_2': 0.0,\r\n    'wrist_3': 0.0,\r\n    'gripper': 0.8            # 0=open, 1=closed\r\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"task-space-actions",children:"Task-Space Actions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# End-effector target\r\nactions = {\r\n    'position': [0.5, 0.0, 0.5],  # x, y, z in meters\r\n    'orientation': [0, 0, 1, 0],  # quaternion\r\n    'gripper': 0.8\r\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"waypoint-sequences",children:"Waypoint Sequences"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Multi-step action sequence\r\nactions = [\r\n    {'position': [0.5, 0.0, 0.3], 'gripper': 1.0},  # Approach\r\n    {'position': [0.5, 0.0, 0.0], 'gripper': 1.0},  # Lower\r\n    {'position': [0.5, 0.0, 0.0], 'gripper': 0.0},  # Release\r\n    {'position': [0.5, 0.0, 0.3], 'gripper': 0.0}   # Retreat\r\n]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"popular-vla-models",children:"Popular VLA Models"}),"\n",(0,r.jsx)(n.h3,{id:"rt-1-robotics-transformer-1",children:"RT-1 (Robotics Transformer-1)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Google's foundational VLA model"}),"\n",(0,r.jsx)(n.li,{children:"Trained on 130,000+ robot episodes"}),"\n",(0,r.jsx)(n.li,{children:"50-60% success rate on unseen tasks"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"rt-2-robotics-transformer-2",children:"RT-2 (Robotics Transformer-2)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Improved with visual reasoning"}),"\n",(0,r.jsx)(n.li,{children:"50% improvement over RT-1"}),"\n",(0,r.jsx)(n.li,{children:"Better generalization to new objects"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"open-source-alternatives",children:"Open-Source Alternatives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MOSAIC"}),": Multi-task robotics model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Flamingo"}),": Vision-language model by DeepMind"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CLIP"}),": OpenAI's vision-language matching"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"vla-vs-traditional-robotics",children:"VLA vs Traditional Robotics"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Traditional"}),(0,r.jsx)(n.th,{children:"VLA"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Programming"})}),(0,r.jsx)(n.td,{children:"Code imperative steps"}),(0,r.jsx)(n.td,{children:"Describe in language"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Generalization"})}),(0,r.jsx)(n.td,{children:"Limited to trained scenarios"}),(0,r.jsx)(n.td,{children:"Generalizes to new scenarios"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Learning"})}),(0,r.jsx)(n.td,{children:"Manual feature engineering"}),(0,r.jsx)(n.td,{children:"End-to-end learning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Flexibility"})}),(0,r.jsx)(n.td,{children:"Task-specific"}),(0,r.jsx)(n.td,{children:"Multi-task capable"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Development Time"})}),(0,r.jsx)(n.td,{children:"Weeks/months"}),(0,r.jsx)(n.td,{children:"Days/weeks"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"current-limitations",children:"Current Limitations"}),"\n",(0,r.jsx)(n.h3,{id:"1-computational-requirements",children:"1. Computational Requirements"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# VLA models are large\r\nmodel_size = 1.2  # Billion parameters\r\ninference_time = 0.5  # seconds per action\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-real-time-performance",children:"2. Real-Time Performance"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"RT-1/RT-2: ~500ms per action"}),"\n",(0,r.jsx)(n.li,{children:"Requires optimization for real-time control"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-sim-to-real-gap",children:"3. Sim-to-Real Gap"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Training in simulation"}),"\n",(0,r.jsx)(n.li,{children:"Deploying on real robots"}),"\n",(0,r.jsx)(n.li,{children:"Domain randomization helps but not perfect"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsx)(n.h3,{id:"1-pick-and-place",children:"1. Pick and Place"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Instruction: "Pick the coffee cup and put it in the dishwasher"\r\nVLA understands: object location, target location, gripper actions\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-manipulation-tasks",children:"2. Manipulation Tasks"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Instruction: "Open the drawer and retrieve the keys"\r\nVLA understands: sequential actions, gripper control\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-assembly-tasks",children:"3. Assembly Tasks"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Instruction: "Assemble the chair: attach leg A to frame, add screws"\r\nVLA understands: multi-step assembly process\n'})}),"\n",(0,r.jsx)(n.h3,{id:"4-mobile-manipulation",children:"4. Mobile Manipulation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Instruction: "Navigate to the kitchen and place items on the counter"\r\nVLA understands: navigation + manipulation\n'})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["Learn about ",(0,r.jsx)(n.a,{href:"training-vla-models",children:"Training VLA Models"})," to build your own vision-language-action system."]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(6540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);